\section{Generalized Additive Model} \label{sec:gam}
In this section, we explore one of the most intuitive methods of fitting a binary-response data set. General additive models (GAM) are a natural extension of generalized linear models (GLM), which accept response(s) distributed as a member of the exponential family through the use of link function, by allowing covariates to be modeled with both parametric specifications and nonparametric techniques. Predictors \texttt{degree}, \texttt{decision\_method}, \texttt{decision\_month}, \texttt{gre}, \texttt{status}, \texttt{uni\_faclty} and \texttt{uni\_pub} are used in this section. A derived variable, \texttt{gre}, is constructed by addition of \texttt{gre\_quant}, \texttt{gre\_verbal} and \texttt{gre\_writing}, based on the assumption that final decisions made may be related to the overall score only.
\subsection{Logistic Regression}
To start with, we consider tje canonical logistic regression, whose performance, evaluated by the area under the \noun{Receiver Operating Characteristic Curve} (AUC), will be regarded as a benchmark in this paper. The value of AUC ranges from 0 to 1, depending on the discriminating power of each model. A larger AUC value indicates better separation of the data, while a value equal to 0.5 implies the model does no better than simply tossing a coin.
\par Parameterization of a good-old logistic regression model is specified below.
\begin{align}
\log\left(\frac{p}{1-p}\right)=\mathbf{x}^\top\boldsymbol{\beta}+\beta_0
\end{align}
where $p=P(Y=1)$ is the probability of the candidate being accepted by the school, and $\mathbf{x}$ is a vector of predictors, which can be either numerical and categorical. 
\par Forward selection is a greedy approach of variable selection by including one independent variable which helps explain the most deviance. This procedure, based on Alkaike's Information Criteria (AIC), measures goodness-of-fit using a penalized maximum likelihood, imposing greater penalty on models with higher complexity. The result suggests a model including all predictors (AIC=5574.28).
\par One may also be interested in whether the two types of degrees have different requirements or selection criteria for candidates with the same academic merits and/or applying under the same category. Based on this motivation, three interaction effects (\texttt{degree:status}, \texttt{degree:gpa} and \texttt{degree:gre}) are consequently added to the model fitted above. However, only \texttt{degree:status} has a significant effect under 5\% level of significance. The inclusion of such effect can be further supported by the AIC statistic, which is brought down to 5570.2. The AUC on test data set is evaluated as 0.716.

\subsection{The Lasso}
Now we approach the variable selection problem again with another popular method, the Lasso \cite{Friedman:2001:ESL}, which is a shrinkage method of the regression coefficients by putting a constraint on the $L_1$-norm of $\boldsymbol{\beta}$. The parameter estimate can be written as the solution to an optimization problem.
\begin{align}
\widehat{\boldsymbol{\beta}}^{\text{lasso}}=\underset{\boldsymbol{\beta}}{\arg\min}\left\{\sum_i\left[\log\left(\frac{p_i}{1-p_i}\right)-\mathbf{x}^\top\boldsymbol{\beta}-\beta_0\right]^2+\lambda\|\boldsymbol{\beta}\|_1\right\}
\end{align}
where $\lambda$ controls the extent to which the regression coefficients are suppressed. In particular, the lasso, as opposed to ridge regression with penalty laid on the $L_2$-norm, has a variable selection power. \Cref{lassopath} shows the regularization path of $\boldsymbol{\beta}$ along its corresponding $L_1$-norm, where each line represents the numerical value of regression coefficient of one predictor. The optimal choice of $\lambda$ is attained by 5-fold cross valication (CV) scheme, whose binomial deviance lies upper one standard error of the minimum choice, as shown in \Cref{lassocv}.

\begin{figure}[htpb]
  \centering
    \begin{subfigure}{.5\textwidth} \centering
    \resizebox{\textwidth}{!}{%
	\input{figs/lassopath.tex}}
	\caption{Regularization path of the Lasso. Lines with different colours correspond to different covariates.}
	\label{lassopath}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth} \centering
	\resizebox{\textwidth}{!}{%
	    \input{figs/lassocv.tex}}
	\caption{Binomial deviance against regularization parameter by 5-fold CV. }
	\label{lassocv}
    \end{subfigure}
	%}
    \caption{Results of the GAM model.}
\end{figure}

\par The AIC of the lasso model is not given automatically by package \texttt{glmnet}. Nonetheless, the computation is tractable theoretically, as the effective degrees of freedom are shown to be the expectation of trace of active-set data matrix \cite{tibshirani:2012:dflasso}. For now, we will resort to AUC as an alternative assessment method. AUC for 5-fold cross validated lasso is 0.7118, smaller than that of ordinary logistic regression, which may indicate a lack of information (dimension) to tackle the problem.

\subsection{Generalized Additive Model}
Apart from the methods described above, we can adapt more flexible techniques in the modeling of covariates, since logit transformation has been done on the response variable. In a regression setting, GAM has the form
\begin{align}
\log\left(\frac{p}{1-p}\right)=\alpha+f_1(x_1)+f_2(x_2)+\dots+f_p(x_p)
\end{align}
for a model of $p$ predictors. Here, we define $f_i$'s as any unspecified functions whose forms are yet to be decided either parametrically or nonparametrially. 
\par Basis expansion is one of the possibilities of data augmentation for capturing non-linear relationship between the predictors and transformed response. A \noun{Natural Cubic Spline} (NCS) \cite{Friedman:2001:ESL} with $K$ nodes ($\tau_1<\tau_2<\dots<\tau_K$) can be represented by, in total, $K$ parametric basis functions.
\begin{align}
f_1(x)=1,\quad f_2(x)=x, \quad \ldots, \quad f_{k+2}(x)=d_k(x)-d_{K-1}(x)
\end{align}
where
\begin{align}
d_{k}=\frac{(x-\tau_k)^3_+-(x-\tau_K)^3_+}{\tau_K-\tau_k},\quad k=1,\dots,K.
\end{align}
NCS is, in fact, ordinary cubic splines with additional constraints on boundary nodes, which assert linearity beyond the range of observed data. This assumption is valid in that cubic splines usually inflate drastically around the boundaries, as little information is available in that region. Categorical variables cannot be modeled by splines and are left as ordinary predictors. The model fitted by integrating NCS (with 3 knots) with GLM gives AIC=5493.7. Under 5\% level of significance, all predictors should be retained in the model, including the interaction effect of \texttt{degree} and \texttt{status}, as suggested previously.
\par Other than parametric modeling, there have been several nonparametric statistical tools which aim at delineating the relationship between the dependent and independent variables in a more flexible manner. One of the most famous techniques is the \noun{Locally Weighted Scatterplot Smoothing} (LOESS) regression \cite{Friedman:2001:ESL}. It takes data points in a neighborhood $\mathcal{N}(x)$ centered at current $x$ and use weighed least square to minimize
\begin{align}
\sum_{i, x_i\in \mathcal{N}(x)}\left[\log\left(\frac{p_i}{1-p_i}\right)-\beta_0-\beta_1(x_i-x)\right]^2W\left(\frac{|x-x_i|}{\delta_x}\right)
\end{align}
for fixed $x$, where $\delta_x=\underset{x_i\in \mathcal{N}(x)}{\max}|x-x_i|$ denotes the diameter of the neighborhood. Then, estimation of fitted values is carried out by traversing the whole linear space. AUC of loess regression (with \texttt{span=0.2}) embedded in the context of GAM is 5570.2. Altering the levels of \texttt{span} does not change much the resulted AIC.
\par We now compute the AUC of the model fitted with NCS, which has a smaller AIC, as 0.7286, which is superior to logistic regression. However, interpretability is hurt for predictors expanded by NCS. Hence, we will only elaborate a few preliminary but interesting findings from the logistic regression. 

\par Here, we omit insignificant levels of categorical predictors for easier analysis. The acceptance rate for \texttt{PhD} application is significantly lower than that for \texttt{MS}. Results released through \texttt{Phone} are more likely to be an offer rather than a rejection letter, followed by \texttt{Email} and \texttt{Website}. This makes sense because important and urgent decisions are often transmitted though more efficient and effective communication tools. U.S. students have a bigger chance to get admitted, compared to students from other backgrounds. While this might be the case, it is worth noticing that international students who apply for \texttt{PhD} degrees has the best shot of admission. Candidates who has higher \texttt{gpa} and \texttt{gre} scores are preferred. Moreover, it is more difficult to secure a place in universities with higher rankings, as anticipated, and those with more faculty members tend to take in more applicants.
\begin{tabular}
\centering
\begin{table}{l|ccc}
\hline
Effect & Estimate & Std. Error & \textit{p}-value \\
\hline
    \texttt{decision\_methodPhone} &  1.846362 & 0.319259 & $7.33\times 10^{-9}$ \\
\texttt{degreePhD:statusInternational} & 0.405409 & 0.162446 & 0.0126 \\
\hline
\end{tabular}
\caption{Some coefficients fitted with logistic regression.}
\end{table}
